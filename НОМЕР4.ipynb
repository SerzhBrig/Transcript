{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JX-95U7OVg6m"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.linalg import eigh as sp_eigh\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def LDA(X, y, n_components):\n",
        "    n_samples, n_features = X.shape\n",
        "    cl, cls_counts = np.unique(y)\n",
        "    priors = cls_counts / n_samples\n",
        "    X_cls_mean = np.array([X[y == cls].mean(axis=0) for cls in cl])\n",
        "    between_cls_deviation = X_cls_mean - X.mean(axis=0)\n",
        "    within_cls_deviation = X - X_cls_mean[y]\n",
        "    Sb = priors * between_cls_deviation.T @ between_cls_deviation\n",
        "    Sw = within_cls_deviation.T @ within_cls_deviation / n_samples\n",
        "    evals, evecs = sp_eigh(Sb, Sw)\n",
        "    dvecs = evecs[:, np.argsort(evals)[::-1]]\n",
        "    w = X_cls_mean @ dvecs @ dvecs.T\n",
        "    bias = np.log(priors) - 0.5 * np.diag(X_cls_mean @ w.T)\n",
        "    if n_components is None:\n",
        "        n_components = min(cl.size - 1, n_features)\n",
        "    return X @ dvecs[:, : n_components]"
      ],
      "metadata": {
        "id": "LWFgTVr6V0_G"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LDA vs PCA\n",
        "simularity:\n",
        "1)банальное, оба метода предназначены для уменьшения размерности данных, что упрощает анализ, визуализацию и снижает вычислительную сложность\n",
        "2)Для обоих методов важно предварительно стандартизировать данные, чтобы признаки имели одинаковую шкалу\n",
        "3)Оба метода проецируют исходные данные на новое подпространство меньшей размерности\n",
        "4) Оба метода находят новые оси, при этом максимизируя какой-то параметр, но эти параметры разные для каждого метода\n",
        "Differences:\n",
        "1) PCA находит оси, которые максимизируют дисперсию данных.А LDA находит оси, которые максимизируют различие между классами и минимизируют разброс внутри классов.\n",
        "2)PCA применяется для предварительной обработки, кластеризации и визуализации. Может использоваться для данных без меток. То есть обучение без учителя.\n",
        "LDA же используется для улучшения разделимости классов в задачах классификации. Требует наличия меток классов. То есть обучение с учителем\n"
      ],
      "metadata": {
        "id": "ZdBNdljMV_AM"
      }
    }
  ]
}